import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from scipy.stats import randint

# 读取数据
file_path = "/Users/yuki/Desktop/数据获取+清理/Preprocessing code/merged_data.csv"  # 确保路径正确
df = pd.read_csv(file_path)

# 按 `FileName` 分组分析
factor_importance_results = {}
num_top_factors = 10  # 选择前 X 个重要因子

for name, group in df.groupby("FileName"):
    group = group.sort_values(by="Date")  # 确保按时间排序
    
    # 计算目标变量：未来90天收益率
    group["future_90d_return"] = (group["Close"].shift(-90) - group["Close"]) / group["Close"]
    group = group.dropna(subset=["future_90d_return"])  # 移除缺失目标值的行
    
    # 选取特征列，排除 `FileName`, `Date`, `future_90d_return`
    feature_cols = [col for col in group.columns if col not in ["FileName", "Date", "future_90d_return"]]
    
    # 处理数据：填充缺失值 & 归一化
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    
    X = pipeline.fit_transform(group[feature_cols])
    y = group["future_90d_return"].values
    
    # 确保 `feature_cols` 与 `X` 长度一致
    processed_feature_cols = group[feature_cols].columns.tolist()
    
    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 随机搜索优化超参数
    param_dist = {
        "n_estimators": randint(50, 300),
        "max_depth": randint(3, 15),
        "min_samples_split": randint(2, 10),
        "min_samples_leaf": randint(1, 5)
    }
    
    rf = RandomForestRegressor(random_state=42)
    search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20, cv=3, n_jobs=-1, random_state=42)
    search.fit(X_train, y_train)
    
    # 使用最佳参数训练最终模型
    best_rf = search.best_estimator_
    best_rf.fit(X_train, y_train)
    
    # 计算因子重要性
    processed_feature_cols = feature_cols[:X.shape[1]]  # 取前 24 个特征
    importance = best_rf.feature_importances_
    importance_df = pd.DataFrame({'Factor': processed_feature_cols, 'Importance': importance})
    importance_df = importance_df.sort_values(by='Importance', ascending=False)
    
    # 存储结果
    factor_importance_results[name] = importance_df
    
    # 绘制因子重要性柱状图
    plt.figure(figsize=(10, 5))
    plt.barh(importance_df['Factor'][:num_top_factors], importance_df['Importance'][:num_top_factors], color='skyblue')
    plt.xlabel("Importance")
    plt.ylabel("Factor")
    plt.title(f"Top {num_top_factors} Important Factors - {name}")
    plt.gca().invert_yaxis()
    plt.show()
