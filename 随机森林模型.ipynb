{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 读取数据\n",
    "#记得修改绝对路径\n",
    "file_path = \"C:\\\\Users\\\\15271\\\\Desktop\\\\量化\\\\数据获取+清理\\\\Preprocessing code\\\\merged_data.csv\"  # 确保路径正确\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 按 `FileName` 分组分析\n",
    "factor_importance_results = {}\n",
    "num_top_factors = 10  # 选择前 X 个重要因子\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group = \n",
    "name = \n",
    "feature_cols=\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def data_classify(group, name):\n",
    "    #group = group.sort_values(by = \"Date\")\n",
    "\n",
    "    # 计算目标变量：未来90天收益率\n",
    "    #group[\"future_30d_return\"] = (group[\"Close\"].shift(-30) - group[\"Close\"]) / group[\"Close\"]\n",
    "    #group = group.dropna(subset=[\"future_30d_return\"])  # 移除缺失目标值的行\n",
    "    \n",
    "    #group.set_index('Date', inplace=True)\n",
    "    # 计算滚动平均\n",
    "    #window_size = 30\n",
    "    #group[\"future_30d_return\"] = group[\"Close\"].rolling(window=window_size).mean()\n",
    "    #group = group.dropna(subset=[\"future_30d_return\"])  # 移除缺失目标值的行\n",
    "    \n",
    "    \n",
    "     # 处理数据：填充缺失值 & 归一化\n",
    "    #pipeline = Pipeline([\n",
    "       # ('imputer', SimpleImputer(strategy='mean')),\n",
    "        #('scaler', StandardScaler())\n",
    "      #  ])\n",
    "   # print(type(pipeline))\n",
    "    #SimpleImputer(strategy='mean'): \n",
    "    # This step replaces missing values in your data with the mean of the respective column\n",
    "    \n",
    "    #StandardScaler(): \n",
    "    # This step standardizes features by removing the mean and scaling to unit variance.\n",
    "    \n",
    "    #selecting feature columns\n",
    "    #feature_cols = [col for col in group.columns if col not in [\"FileName\", \"Date\", \"future_30d_return\"]]\n",
    "      \n",
    "    #处理数据：填充缺失值 & 归一化\n",
    "    #X = pipeline.fit_transform(group[feature_cols])\n",
    "   # y = group[\"future_30d_return\"].values\n",
    "    \n",
    "    # 确保 `feature_cols` 与 `X` 长度一致\n",
    "    #feature_cols = group[feature_cols].columns.tolist()\n",
    "    #print(group.shape[1])\n",
    "    #return X, y, feature_cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def data_classify(group, name):\n",
    "    group = group.sort_values(by=\"Date\")\n",
    "\n",
    "    # 计算未来30天收益率\n",
    "    group[\"future_30d_return\"] = (group[\"Close\"].shift(-30) - group[\"Close\"]) / group[\"Close\"]\n",
    "    group = group.dropna(subset=[\"future_30d_return\"])  # 移除缺失目标值的行\n",
    "    \n",
    "    # 设置索引为日期\n",
    "    group.set_index('Date', inplace=True)\n",
    "\n",
    "    # 创建特征和目标变量\n",
    "    window_size = 30\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(group) - window_size - 30):  # 留出最后30天进行预测\n",
    "        X = group.iloc[i:i + window_size][\"Close\"].values  # 提取窗口内的收盘价\n",
    "        y = group[\"future_30d_return\"].iloc[i + window_size]  # 目标是窗口结束后的收益率\n",
    "        features.append(X)\n",
    "        targets.append(y)\n",
    "\n",
    "    # 转换为DataFrame\n",
    "    X = pd.DataFrame(features)\n",
    "    y = pd.Series(targets)\n",
    "\n",
    "    # 处理数据：填充缺失值 & 归一化\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # 处理数据：填充缺失值 & 归一化\n",
    "    X = pipeline.fit_transform(X)\n",
    "    \n",
    "    # 确保特征列与 X 长度一致\n",
    "    feature_cols = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    print(\"Features shape:\", X.shape)\n",
    "    print(\"Targets shape:\", y.shape)\n",
    "    \n",
    "    return X, y, feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partameter distribution for a hyperparameter tuning process\n",
    "\n",
    "    n_estimators: The number of trees in the forest\n",
    "\n",
    "    max_depth: The maximum depth of each tree, controlliing how deep the trees can grow\n",
    "\n",
    "    min_sample_split: The minimum number of samples required to split and internal node.\n",
    "\n",
    "    min_samples_leaf:The minimum nuber of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor():\n",
    "    n_estimators: int, default = 100\n",
    "\n",
    "    criterion: str, default = \"squared_error\", 也可以是\"absolute_error\",\"poisson\"泊松回归（仅用于非负目标变量）\n",
    "        poisson: ???\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ (X_train, y_train, best_rf):  #参数：交叉验证折数\n",
    "    cv_folds = 5\n",
    "    scoring = \"r2\"\n",
    "    \n",
    "    #交叉检验，评估模型性能\n",
    "    scores = cross_val_score(best_rf, X_train, y_train, cv=cv_folds, scoring=scoring)  \n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ridge(X_train, y_train, model):\n",
    "    #规范正则化强度\n",
    "    alpha = 0.1\n",
    "    \n",
    "    model = Ridge(alpha = alpha)  # alpha 是正则化强度\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def testing(X_test, y_test, best_rf):\n",
    "# 使用测试集进行预测\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    # 计算均方误差 (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # 计算 R²（决定系数）\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE) on Test Set: {mse}\")\n",
    "    print(f\"R² Score on Test Set: {r2}\")\n",
    "    \n",
    "    return r2, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, y, feature_cols):\n",
    "     # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # partameter distribution for a hyperparameter tuning process\n",
    "    param_dist = {\n",
    "        \"n_estimators\": randint(200, 300),\n",
    "        \"max_depth\": randint(10, 20),\n",
    "        \"min_samples_split\": randint(9, 11),\n",
    "        \"min_samples_leaf\": randint(1,4)\n",
    "    }\n",
    "    \n",
    "    #Creating a randon forest model\n",
    "    rf = RandomForestRegressor(random_state=42,max_features='sqrt')\n",
    "    \n",
    "    #Trying to optimize 超参数，随机搜索以优化随机森林模型超参数，相当于这只是一个model，但是需要后续输入数据进行训练\n",
    "    search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20, cv=7, n_jobs=-1, random_state=42)\n",
    "    #Training the model (这里相当于把数据输进去进行训练)\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    # 使用最佳参数训练最终模型\n",
    "    best_rf = search.best_estimator_\n",
    "      #这里找到最佳超参数组合，此时这个模型已经经过交叉验证\n",
    "      #但是这个是基于折叠出来的部分数据上进行训练的，因此没有在整个数据集上面进行训练\n",
    "      #所以下面这一步就是要再在整个数据集上面再训练一次\n",
    "    best_rf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "      # 计算因子重要性\n",
    "    processed_feature_cols = feature_cols[:X.shape[1]]  # 除了return以外的所有值\n",
    "    \n",
    "      #X.shape[1]返回的是特征矩阵的列数，\n",
    "      #如果X = np.array([1,2],[3,4]), feature_cols = [\"f1\", \"f2\", \"f3\", \"f4\"]\n",
    "      #X.shape == np.array(2,2)\n",
    "      # feature_cols[ : X.shape[1]] 返回的是第一列和第二列\n",
    "      #X.shape[1]可以让我们直接选取X的所有列\n",
    "    \n",
    "    #获取特征重要性（各个特征对模型预测结果的贡献程度）\n",
    "    #importance 返回的是numpy数组\n",
    "    importance = best_rf.feature_importances_\n",
    "    \n",
    "    #嵌套dataframe\n",
    "    importance_df = pd.DataFrame({'Factor': processed_feature_cols, 'Importance': importance})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    ##交叉检验\n",
    "    #参数：交叉验证折数：\n",
    "    cv_folds = 5\n",
    "    scoring = \"r2\"\n",
    "    \n",
    "    #交叉检验，评估模型性能\n",
    "    scores = cross_val_score(best_rf, X_train, y_train, cv=cv_folds, scoring=scoring) \n",
    "     \n",
    "    return importance_df, best_rf, scores, X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过拟合：训练数据上表现得好，但是test上performance差，捕捉到训练数据上的噪声和细节，但没学习到普遍规律\n",
    "\n",
    "欠拟合：表现都不好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (465, 30)\n",
      "Targets shape: (465,)\n",
      "The score of the model is  [0.52452124 0.56775339 0.64493342 0.53989183 0.3987508 ]\n",
      "Mean Squared Error (MSE) on Test Set: 0.00370726789381344\n",
      "R² Score on Test Set: 0.5372160457830729\n",
      "Features shape: (465, 30)\n",
      "Targets shape: (465,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m X, y, feature_cols \u001b[38;5;241m=\u001b[39m data_classify(group, name)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m importance_df, best_rf, scores, X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m training(X, y , feature_cols)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score of the model is \u001b[39m\u001b[38;5;124m\"\u001b[39m, scores)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#testing\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[80], line 19\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(X, y, feature_cols)\u001b[0m\n\u001b[0;32m     17\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(rf, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#Training the model (这里相当于把数据输进去进行训练)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 使用最佳参数训练最终模型\u001b[39;00m\n\u001b[0;32m     23\u001b[0m best_rf \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1915\u001b[0m         ParameterSampler(\n\u001b[0;32m   1916\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1917\u001b[0m         )\n\u001b[0;32m   1918\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "factors = pd.DataFrame(columns= [\"FileName\", \"Best_rf\", \" Importance_dataframe\", \"Parameter\", \"training_R2.mean\", \"testing_R2\"])\n",
    "\n",
    "for name, group in df.groupby(\"FileName\"):\n",
    "  #Data modification\n",
    "  X, y, feature_cols = data_classify(group, name)\n",
    " \n",
    "  #training\n",
    "  importance_df, best_rf, scores, X_train, X_test, y_train, y_test = training(X, y , feature_cols)\n",
    "  print(\"The score of the model is \", scores)\n",
    " \n",
    "  #testing\n",
    "  r2, mse = testing(X_test, y_test, best_rf)\n",
    "  \n",
    " # while (True):\n",
    "   # if mse > 0.005 and scores.mean() > 0.6:\n",
    "      #break\n",
    "    #else: \n",
    "      #正则化弱化噪声因子\n",
    "     # best_rf = ridge(X_train, y_train, best_rf)\n",
    "      \n",
    "     \n",
    "     # scores = cross_ (X_train, y_train, best_rf)\n",
    "      #重新test 新的model\n",
    "      #r2, mse = testing(X_test, y_test, best_rf)\n",
    "      \n",
    "  #生成储存超参数的dataframe\n",
    "  parameter = pd.DataFrame(columns=[\"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\"])\n",
    "  parameter.loc[len(parameter)] = {\"n_estimators\": best_rf.n_estimators, \"max_depth\": best_rf.max_depth, \"min_samples_split\": best_rf.min_samples_split, \"min_samples_leaf\": best_rf.min_samples_leaf}\n",
    "  #重新挑选因子\n",
    "    # 计算因子重要性\n",
    "  processed_feature_cols = feature_cols[:X.shape[1]]  # 除了return以外的所有值\n",
    "    \n",
    "    #X.shape[1]返回的是特征矩阵的列数，\n",
    "    #如果X = np.array([1,2],[3,4]), feature_cols = [\"f1\", \"f2\", \"f3\", \"f4\"]\n",
    "    #X.shape == np.array(2,2)\n",
    "    # feature_cols[ : X.shape[1]] 返回的是第一列和第二列\n",
    "    #X.shape[1]可以让我们直接选取X的所有列\n",
    "    \n",
    "    #获取特征重要性（各个特征对模型预测结果的贡献程度）\n",
    "    #importance 返回的是numpy数组\n",
    "  importance = best_rf.feature_importances_\n",
    "    \n",
    "    #嵌套dataframe\n",
    "  importance_df = pd.DataFrame({'Factor': processed_feature_cols, 'Importance': importance})\n",
    "  importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "  factors.loc[len(factors)] = {\"FileName\": name, \"Best_rf\":best_rf, \" Importance_dataframe\": importance_df, \"Parameter\": parameter, \"training_R2.mean\": scores.mean(), \"testing_R2\": r2}\n",
    "  #print(factors.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors.info()\n",
    "\n",
    "#用pickle导出dataframe\n",
    "\n",
    "factors.to_pickle(\"未来三十天随机森林(1).pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
